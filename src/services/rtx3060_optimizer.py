"""
ReZ-Acceleration - RTX 3060 12GB Ultimate Optimizer
4-bit QLoRA • FlashAttention-2 • Unsloth • xFormers • BF16
"""

import torch
import os
import sys
from pathlib import Path

class RTX3060Optimizer:
    """Complete optimization suite for NVIDIA RTX 3060 12GB"""
    
    def __init__(self, vram_budget_gb=9):
        self.vram_budget = vram_budget_gb
        self.device = None
        self.setup_environment()
    
    def setup_environment(self):
        """Configure environment for optimal RTX 3060 performance"""
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = f"max_split_size_mb:{self.vram_budget*1024//4}"
        os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        
        if torch.cuda.is_available():
            self.device = torch.cuda.current_device()
            props = torch.cuda.get_device_properties(self.device)
            print(f"✅ GPU: {props.name}")
            print(f"📊 VRAM: {props.total_memory / 1024**3:.1f}GB (Budget: {self.vram_budget}GB)")
            print(f"🎯 Compute Capability: {props.major}.{props.minor}")
            return True
        return False
    
    def get_4bit_config(self):
        """Get 4-bit QLoRA configuration for 70B models on 12GB"""
        from transformers import BitsAndBytesConfig
        import torch
        
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_quant_storage=torch.uint8,
        )
    
    def optimize_training_args(self, batch_size=None):
        """Get optimized training arguments for RTX 3060"""
        from transformers import TrainingArguments
        
        if batch_size is None:
            batch_size = self.get_optimal_batch_size()
        
        return TrainingArguments(
            output_dir="./rtx3060_output",
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=4,
            warmup_steps=100,
            num_train_epochs=3,
            learning_rate=2e-4,
            fp16=False,
            bf16=True,  # RTX 3060 supports BF16!
            logging_steps=10,
            save_strategy="epoch",
            evaluation_strategy="steps",
            eval_steps=500,
            save_total_limit=2,
            remove_unused_columns=False,
            report_to="none",
            gradient_checkpointing=True,
            optim="paged_adamw_8bit",
        )
    
    def get_optimal_batch_size(self, model_size_gb=7):
        """Calculate optimal batch size for RTX 3060"""
        available = self.vram_budget * 0.8  # 20% safety margin
        max_batch = int(available / model_size_gb)
        return max(1, min(2, max_batch))  # RTX 3060 sweet spot
    
    def enable_flash_attention(self, model):
        """Enable FlashAttention-2 if available"""
        try:
            from flash_attn import flash_attn_func
            model.config.use_flash_attention_2 = True
            print("✅ FlashAttention-2 enabled (2x faster)")
            return model
        except:
            print("⚠️ FlashAttention-2 not available")
            return model
    
    def enable_unsloth(self, model, tokenizer, max_seq_length=4096):
        """Enable Unsloth for 4x faster training"""
        try:
            from unsloth import FastLanguageModel
            model = FastLanguageModel.get_peft_model(
                model,
                r=16,
                target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                              "gate_proj", "up_proj", "down_proj"],
                lora_alpha=16,
                lora_dropout=0,
                bias="none",
                use_gradient_checkpointing=True,
                random_state=42,
                max_seq_length=max_seq_length,
            )
            print("✅ Unsloth enabled (4x faster)")
            return model
        except:
            print("⚠️ Unsloth not available")
            return model

# Global instance
optimizer = RTX3060Optimizer()

def optimize_for_rtx3060(vram_budget=9):
    """Convenience function for quick optimization"""
    global optimizer
    optimizer = RTX3060Optimizer(vram_budget)
    return optimizer

print("⚡ ReZ-Acceleration RTX 3060 Optimizer loaded")
